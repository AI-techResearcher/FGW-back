\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}

\begin{document}
\section*{Reading}
Estimating Value at Risk (VaR)

Consider JAC Fund, which has accumulated a position of 50,000 shares of an exchange-traded fund (ETF) that tracks the S\&P 500. This hypothetical ETF trades at \$20 per share for a total holding of $\$ 1$ million. JAC Fund wishes to know how much money could be lost if the ETF fell in value. The theoretical answer is that the fund could lose the entire $\$ 1$ million, under the highly unlikely scenario that the ETF becomes completely worthless.

JAC Fund's management realizes that to make this number meaningful, they must specify a length of time and a probability of certainty. Thus, they might ask how much is the most money that could be expected to be lost $99 \%$ of the time over a 10 -business-day interval. A reasonable answer to that question is $\$ 100,000$. In other words, 99 times out of 100, the position in the ETF will do better than losing $\$ 100,000$ over 10 business days. But on average, during one two-week period out of every 100 such periods, JAC Fund should expect to lose $\$ 100,000$ or more. It could compute other VaR estimates using time horizons other than 10 days $(1,2,5$, and 30 days are also common) or probabilities other than $99 \%$ (90\%, 95\%, and $98 \%$ are also common).

It is easy to assume that VaR analysis is based on the normal probability distribution, because most VaR applications use the statistics of that distribution. However, VaR computation does not require the use of a normal probability distribution or any other formal probability distribution. It merely requires some method or model of predicting the magnitudes and probabilities of various loss levels.

For example, debt securities do not offer a payout at maturity that is normally distributed. Rather, there is usually a high probability that the debt will be paid off in full and various probabilities that only partial payments will be received. If the potential losses of a position or a portfolio of positions cannot be modeled accurately using the normal distribution or another common distribution, the VaR can be estimated in other ways. If the potential losses form a normal distribution, then a parametric approach can be used.

The following section details the parametric estimation of VaR when the losses are normally distributed. Later sections discuss other methods of estimating VaR.

\section*{Estimating VaR with Normally Distributed Returns}
If the potential losses being analyzed follow a normal distribution, a parametric approach can be used (i.e., VaR can be based on the parameters of the normal distribution). A VaR computation assuming normality and using the statistics of the normal distribution is known as parametric VaR. Computing parametric VaR begins with estimating a standard deviation and inserting the standard deviation into the following formula based on daily price changes:


\begin{equation*}
\text { Parametric } \mathrm{VaR}=N \times \sigma \sqrt{\text { Days }} \times \text { Value } \tag{1}
\end{equation*}


The formula can use time periods other than days by adjusting $N$ and $\sigma$. For simplicity, the formula assumes that the expected return of the investment is zero. The four components to this formula are:

\begin{enumerate}
  \item $N$ is the number of standard deviations, which depends on the confidence level that is specified. The value 2.33 should be used if the user wants to be $99 \%$ confident, 1.65 should be used if the user wants to be $95 \%$ confident, and so forth, with values that can be found using tables or spreadsheets of confidence intervals based on the normal distribution.
  \item $\sigma$ is the estimated daily standard deviation expressed as a proportion of price or value (return standard deviation). The standard deviation is a measure of the volatility of the value. For example, the ETF discussed earlier might be viewed in a particular market as having a daily standard deviation of perhaps $1.35 \%$. Given a stock price of $\$ 20$, we can think of the ETF's daily standard deviation measured in absolute terms as being about $\$ 0.27$. If the standard deviation is expressed as a dollar value of the entire position, then the formula would omit the last term. The standard deviation can be estimated using historical data, observed through option volatilities, or forecasted in some other way, such as with fundamental analysis.
  \item $\sqrt{\text { Days }}$ is the square root of the number of days used for the VaR analysis, such as $1,2,5$, or 10 days. The reason we use the square root is that risk as measured by VaR often grows proportionally with the square root of time, assuming no autocorrelation. Thus, a two-day VaR is only $41.4 \%$ bigger than a one-day VaR. 4. Value is the market value of the position for which the VaR is being computed. For example, it might be the value of a portfolio.
\end{enumerate}

In many cases, such as in this case of a single position, these four inputs are simply multiplied together to find the VaR. In other cases, a further adjustment might be necessary, such as subtracting the collateral that is being held against the potential loss to find the amount that is at risk, or adjusting for the expected profit on the position over the time interval.

\section*{Estimating VaR with Normally Distributed Underlying Factors}
The approach just described is the simplest case of the analytic approach to computing VaR. Note that the standard deviation used in the equation was the standard deviation of the value or returns of the position being studied. In more complex examples of the analytic approach, the values being studied (e.g., security prices) are modeled as functions of one or more underlying economic variables or factors, such as when an option price is modeled as a function of five or more variables. In these instances, the VaR equation is expressed using the volatilities and correlations of the underlying factors, as well as the sensitivity of the security prices to those factors.

In the case of highly nonlinear price functions, such as options, the sensitivities include terms to capture the nonlinearity of large movements (e.g., by using convexity). Thus, the parametric VaR equation that is rather simple for a single position with value changes that are normally distributed can become quite complex for positions with highly nonlinear relationships to underlying factors and/or positions that depend on several factors.

\section*{Two Primary Approaches to Estimating the Volatility for VaR}
In most parametric VaR applications, the biggest challenge is estimating the volatility of the asset containing the risk. A common approach is to estimate the standard deviation as being equal to the asset's historical standard deviation of returns. Much work has been done and is being devoted to developing improved forecasts of volatility using past data. These efforts focus on the extent to which more recent returns should be given a higher weight than returns from many time periods ago. Models such as ARCH and GARCH emphasize more recent observations in estimating volatility based on past data and were discussed in the Statistical Foundations session.

Another method of forecasting volatility is based on market prices of options. Estimates of volatility are based on the implied volatilities from option prices. These estimates, when available and practical, are typically more accurate than estimates based on past data, since they reflect expectations of the future. For example, in our case of estimating the VaR on a position linked to an ETF tracking the S\&P 500, the analyst may use implied volatilities from options on products that track the S\&P 500 or may examine the CBOE Volatility Index (VIX) futures contract that reflects S\&P 500 volatility.

\section*{Two Approaches to Estimating VaR for Leptokurtic Positions}
The VaR computations are sensitive to misestimation of the probabilities of highly unusual events. If a position's risk is well described by the normal distribution, then the probabilities of extreme events are easily determined using an estimate of volatility. But leptokurtic positions have fatter tails than the normal distribution, so VaR is sensitive to the degree to which the position's actual tails exceed the tails of a normal distribution.

One solution is to use a probability distribution that allows for fatter tails. For example, the $t$-distribution not only allows for fatter tails than the normal distribution but also has a parameter that can be adjusted to alter the fatness of the tails. Also, the lognormal distribution is often viewed as providing a more accurate VaR for skewed distributions. Some applications involve rather complicated statistical probability distributions, such as mixed distributions, to incorporate higher probabilities of large price changes. In these cases, the parametric VaR (Equation 1) must be modified to reflect the new probability distribution.

A second and potentially simpler approach to adjusting for fat tails is simply to increase the number of standard deviations in the formula for a given confidence level. The increase should be based on analysis (typically historical analysis) of the extent of the kurtosis. An analyst computing VaR for a $99 \%$ confidence level might adjust the number of standard deviations in the VaR computation from the 2.33 value that is derived using a normal distribution to a value reflecting fatter tails, such as 2.70. The higher value would likely be based on empirical analysis of the size of the tails in historical data for the given position or similar positions. It is usually necessary to adjust the number of standard deviations only in the cases of very high confidence levels, since most financial return distributions are reasonably close to being normally distributed within 2 standard deviations of the mean. The adjustment may need to be large for very high confidence levels that focus on highly unusual outcomes. Extreme value theory is often used to provide estimates of extremely unlikely outcomes.

\section*{Estimating VaR Directly from Historical Data}
Rather than using a parameter such as the standard deviation to compute a parametric VaR, a very simple way to estimate VaR can be to view a large collection of previous price changes and compute the size of the price change for which the specified percentage of outcomes was lower.

For example, consider a data set with a long-term history of deviations of an ETF's return from its mean return. We wish to estimate a five-day $99 \%$ VaR. We might collect the daily percentage price changes of the ETF for the past 5,000 days and use them to form 1,000 periods of five days each. We then rank the five-day deviations from the highest to the lowest. Suppose we find that exactly 10 of these 1,000 periods had price drops of more than $6.8 \%$, and all the rest of the periods (99\%) had better performance. The $99 \%$ five-day VaR for our ETF position could then be estimated at $6.8 \%$ of the portfolio's current value, under the assumption that past price changes are representative of future price changes.

The value of this approach is its conceptual simplicity, its computational simplicity, and the fact that it works even if the underlying probability distribution is unknown. The approach requires the process to be stable, meaning that the risk of the assets hasn't changed and that the number of past observations is sufficiently large to make an accurate estimate. The requirement of unchanging asset risk throughout the many previous observation periods usually disqualifies this approach for derivatives and some alternative investments with dynamically changing risk exposure, such as hedge funds. The requirement of sufficient past observations is a challenge for illiquid alternative investments, such as private real estate and private equity.

\section*{Estimating VaR with Monte Carlo Analysis}
Monte Carlo analysis is a type of simulation in which many potential paths of the future are projected using an assumed model, the results of which are analyzed as an approximation to the future probability distributions. It is used in difficult problems when it is not practical to find expected values and standard deviations using mathematical solutions.

An example outside of investments illustrates the method. An analyst might be trying to figure out the best strategy for playing blackjack at a casino, such as whether a gambler should "stay" at 16 or 17 when the dealer has a face card showing. Solving this problem with math and statistics can get so complex that it may be easier to simulate the potential strategies. To perform a Monte Carlo analysis, a computer program is designed to simulate how much money the gambler would win or lose if the gambler played thousands and thousands of hands with a given strategy. The computer simulates play for thousands of games, one at a time, using the known probabilities of drawing various cards. The strategy that performs best in the simulations is then viewed as the strategy that will work best in the future.

In finance, it can be very complex to use a model to solve directly for the probability of a given loss in a complex portfolio that experiences a variety of market events, such as interest rate shifts. To address the problem with Monte Carlo simulation, the risk manager defines how the market parameters, such as interest rates, might behave over the future and then programs a computer to project thousands and thousands of possible scenarios of interest rate changes and other market outcomes. Each scenario is then used to estimate results in terms of the financial outcomes on the portfolio being analyzed. These results are then used to form a frequency distribution of value changes and estimate a VaR. A Monte Carlo simulation might project one million outcomes to a portfolio. In that case, a 99\% VaR would be the loss that occurred in the 10,000th worst outcome.

\section*{Three Scenarios for Aggregating VaR}
Once VaR has been computed for each asset or asset type, how are the VaRs aggregated into a VaR for the entire portfolio? For example, consider a hedge fund with equally weighted allocations to its only two positions. The fund's analyst reports a VaR of $\$ 100,000$ for position \#1 and a VaR of $\$ 100,000$ for position \#2. The critical question is the VaR of the combined positions. Let's consider three scenarios based on correlations between the returns of the two positions:

\begin{enumerate}
  \item Perfect positive correlation: If the two positions are identical or have perfectly positive correlated and identical risk exposures, then the VaR of the combination is simply the sum of the individual VaRs, $\$ 200,000$.

  \item Zero correlation: If the two positions have statistically independent risk exposures, then under some assumptions, such as normally distributed outcomes, the VaR of the combination might be the square root of the sum of the squared individual VaRs, or $\$ 141,421$, which can be derived from the equation for the variance of uncorrelated normally distributed returns and the formula for parametric VaR based on the normal distribution.

  \item Perfect negative correlation: If the two positions completely hedge each other's risk exposures, then the VaR of the combination would be $\$ 0$.

\end{enumerate}

Thus, VaRs should be added together to form a more global VaR only when the risks underlying the individual VaRs are perfectly correlated and have identical risk exposures. In other words, the addition of VaRs assumes that every asset or position will experience a highly abnormal circumstance on the same day. If the risks of the assets or positions are imperfectly correlated with each other, the VaRs should be combined using a model that incorporates the effects of diversification using statistics and the correlation between the risks.


\end{document}