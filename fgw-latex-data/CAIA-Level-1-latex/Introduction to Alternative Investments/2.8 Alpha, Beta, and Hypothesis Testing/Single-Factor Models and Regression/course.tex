\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}

\begin{document}
\section*{Reading}
Single-Factor Models and Regression

The session Financial Economics Foundations discussed single-factor equity pricing models. The best-known single-factor market model is the capital asset pricing model (CAPM), which states that the expected return and realized return of an asset are linearly related to its market beta. This section begins by detailing the application of simple linear regression to the ex post version of the single-factor market model.

\section*{Simple Linear Regression and the Single-Factor Market Model}
A regression is a statistical analysis of the relationship that explains the values of a dependent variable as a function of the values of one or more indepentent variables based on a specified model. The dependent variable is the variable supplied by the researcher that is the focus of the analysis and is determined at least in part by other (independent or explanatory) variables. Independent variables are those explanatory variables that are inputs to the regression and are viewed as causing the observed values of the dependent variable.

In a linear regression, the model that describes the relationship between the dependent variable and the independent variable or variables is linear. A simple linear regression is a linear regression in which the model has only one independent variable. For example, the ex post version of the single-factor market model describes realized excess returns of a security or fund as a linear function of an intercept, the market beta, the market portfolio's realized excess return, and an error term that reflects idiosyncratic risk. An excess return is a total return minus the periodic riskless rate. The single-factor market-model-based regression equation for asset $i$, based on a time series of total return data, is as follows:

\section*{$\xrightarrow[\text { E }]{\rightarrow}$ EQUATION EXCEPTION LIST}

\begin{equation*}
R_{i t}-R_{f}=\alpha_{i}+b_{i m}\left(R_{m t}-R_{f}\right)+e_{i t} \tag{1}
\end{equation*}


Where $R_{i t}$ is the return of asset $i$ in time period $t, R_{f}$ is the periodic riskless rate, $\alpha_{i}$ is the estimated intercept, $b_{i m}$ is the estimated slope coefficient, $R_{m t}$ is the return of the market portfolio in time period $t$, and $e_{i t}$ is the residual or estimated error term for asset $i$ at time $t$.

Equation 1 seeks to predict or explain the values of the dependent variable, excess returns $E\left(R_{i t}\right)-R_{f}$, through movements in the independent variable, the excess return of the market portfolio $\left(R_{m t}-R_{f}\right), b_{i m}$ is the estimated slope coefficient of the regression and is an estimate of the beta for asset $i$. The slope coefficient is a measure of the change in a dependent variable with respect to a change in an independent variable. In this example, the slope coefficient estimates the linear sensitivity of the return of asset $i$ to the excess return of the market. The estimate of the intercept of the regression is $\alpha_{i}$. The intercept is the value of the depent variable when all independent variables are zero. In the case of Equation 1, the intercept can be interpreted as an estimate of the average ex post alpha of asset $i$. Finally, the residuals of the regression, $e_{i t}$, reflect the regression's estimate of the idiosyncratic portion of asset is realized returns above or below its mean idiosyncratic return (i.e., the regression's estimates of the error term).

\section*{Ordinary Least Squares Regression}
There are unlimited estimated values that can be inserted for the intercept $\left(\alpha_{i}\right)$ and slope coefficient $\left(b_{i m}\right)$ in Equation 1 . Ordinary least squares regression, the most common regression procedure, selects the intercept and slope that minimize the sum of the squared values of the residuals (the values of $e_{i t}$ ). In simple linear regression, the process may be envisioned as drawing a regression line through a scatter plot of the dependent variable and independent variable. The vertical distance between the regression line and each observation is the residual. The least squares fitting criterion minimizes the sum of those distances squared. The use of ordinary least squares has several advantages: It is quick and easy, and the slope coefficient that results has an intuitive interpretation. Least squares regression has been shown to generate unbiased and most likely estimates of the slope coefficient and intercept if the error terms in the model are (1) normally distributed, (2) uncorrelated, and (3) homoskedastic (i.e., having the same finite variance). Violations of these assumptions are discussed in the next three sections. Other criteria for fitting a model to data also exist.

\section*{Outliers}
Violations of the assumption that the error term in the model is normally distributed often occur when the data are subject to very large outliers, as is often the case in investment returns.

Problem 1: Outliers. Fat tails (leptokurtic distributions) are synonymous with frequent outliers. Alternative investment returns are especially prone to being leptokurtic. Large outliers dominate a regression, potentially causing the estimates of the slope and intercept to be driven too much by the outliers, rather than by the remaining, more representative data. Ordinary least squares regression seeks to minimize the sum of squared residuals, and the squaring of residuals can cause outliers to have disproportionately higher influence than observations closer to the mean.

Response 1: A critical but often overlooked task in linear regression is visual observation of the residuals of the regression. At least two plots are advisable for important regressions. Residuals should be plotted on the vertical axis against the independent or explanatory variable on the horizontal axis, and time-series residuals should be plotted on the vertical axis against time on the horizontal axis. The analyst should note extreme outliers to determine if the residuals reflect data errors or economic fact. If the extreme residuals are not the result of errors, the analyst should determine if the underlying economic behavior causing the observation warrants the large level of influence that the outlier has on the estimated parameters. If the outlier is caused by an event that can be reasonably expected to not recur, perhaps the outlier should be removed. An example is a fund experiencing a catastrophic event from short option positions that has amended its investment strategy to disallow short option positions. It is important not to remove outliers corresponding to gains or losses that are likely to be repeated.

For example, if an analyst regressed the monthly returns of a U.S. financial stock on U.S. stock market returns over a period including 2007 and 2008 , the analyst would probably obtain a very high estimate of the stock's beta due especially to the months in which financial stocks experienced tremendously negative returns and in which the overall market experienced negative returns as well. The analyst would detect these outliers with a plot and then need to decide whether the observed correlations were a representative sample on which to forecast future systematic risk (beta) or the outliers generated an estimate of beta that is unduly indicative of behavior under stressed conditions and therefore unrepresentative of anticipated market conditions.

\section*{Autocorrelation}
The simplest statistical regression procedures assume that the model's error terms are uncorrelated-including through time. Autocorrelation of the error terms is a violation of that assumption.

Problem 2: Autocorrelation. Violations of the assumption that the error term is uncorrelated through time most often occur when returns are autocorrelated. Many alternative investment return series are especially prone to autocorrelation due to smoothed pricing or illiquidity.

Response 2: The Durbin-Watson statistic, detailed in the Statistical Foundations session, is used to test for autocorrelation of residuals. If the Durbin-Watson statistic indicates autocorrelation, there are several well-established statistical procedures for performing adjusted regressions that provide better results. First-order autocorrelation is a common phenomenon in alternative investments and is reasonably easy to address. For example, if an analyst regresses the percentage changes in a real estate project's value based on monthly appraisals against the overall market return, the residuals of the regression might exhibit autocorrelation based on a Durbin-Watson test. The autocorrelation may indicate that the appraisal valuations were reflecting value changes on a delayed basis. Such a regression should be corrected for autocorrelation in order to provide a more accurate measure of the correlation between true real estate values and the overall market.

\section*{Heteroskedasticity}
The simplest statistical regression procedures assume that the variance of the model's error terms is homoskedastic.

Problem 3: Heteroskedasticity: Heteroskedasticity is the opposite of homoskedasticity. In a regression, heteroskedasticity refers to a situation in which the variance of the error term varies. For example, the variance of the error term may be correlated with an independent variable, may vary through time, or may be related to some other variable or dimension. With homoscedasticity, the variance of the error term is constant.

Response 3: The same plots used for outlier examination should be used to detect heteroskedasticity (i.e., residuals should be plotted against the independent variable and against time). In this visual analysis, the analyst should look for a pattern in the dispersion of the residuals, such as a $<$, >, <>, or $><$ pattern. For example, a < pattern would show generally increasing dispersion of the residuals moving from left to right in the diagram. Heteroskedasticity can be formally detected using various tests. The problem with regression results from data exhibiting heteroskedasticity is that the estimated regression parameters are unduly influenced by the data related to the greatest variance in the error term. The most popular correction is weighted least squares, in which a weighting scheme is developed and applied to the data to reduce the importance of the data subject to higher error-term volatility.

For example, an analyst regressed the returns of a corporate bond against a constant maturity Treasury index. A plot of the residuals through time tends to indicate a> pattern, with earlier observations (to the left) having more dispersion than more recent observations (to the right). The heteroskedasticity is attributable to the declining price volatility of the corporate bond as its maturity nears and its duration declines. The earliest observations with the highest dispersion dominate the regression, generating inefficient estimates. A weighted least squares approach should be used to adjust the influence of the observations toward being more equal over time.

In summary, the accuracy of a regression's results may be adversely affected by three primary issues: outliers, autocorrelation, and heteroskedasticity. The statistical approach should be adjusted as necessary to correct for any of these challenges before using the estimated parameters.

\section*{Interpreting a Regression's Goodness of Fit}
The first major interpretation of a regression's results is evaluating the overall explanatory power of the regression. The explanatory power of the regression is evaluated as its goodness of fit. The goodness of fit of a regression is the extent to which the model appears to explain the variation in the dependent variable. The $r$ squared value of the regression, which is also called the coefficient of determination, is often used to assess goodness of fit, especially when comparing models. In a simple linear regression, the $r$-squared is simply the squared value of the estimated correlation coefficient between the dependent variable and the independent variable. Correlation, discussed in the Statistical Foundations session, ranges from -1 to +1 , with negative values showing an inverse relationship between two variables, and positive values denoting a direct relationship between two variables. Because the $r$-squared is equal to a correlation coefficient squared, the range of possible values for $r$-squared is between zero and 1 and is often expressed as a percentage. When building or explaining financial relationships, larger values of $r$ squared are preferred, everything else being equal, as the independent variable is explaining a greater portion of the variance in the dependent variable.

$r$-squared is also interpreted in an absolute sense. For example, a long-only mutual fund may have an $r$-squared of perhaps 0.90 (i.e., $90 \%$ ) in a regression of its returns on the returns of a market index. An $r$-squared such as 0.90 would often be described as meaning that the independent variable (in this case, the returns of the market index) explained $90 \%$ of the variation in the dependent variable (in this case, the returns to the mutual fund). This can be interpreted as indicating that $90 \%$ of the fund's returns were explained by the systematic risk (i.e., exposure to the market risk represented by the index). The remaining value, $1-r^{2}$, is the idiosyncratic risk, or the risk that is not explained by the market index. In this case, the idiosyncratic risk is $10 \%$ of the fund's total risk. The fund's idiosyncratic risk might be due to incomplete diversification, such as holding only 25 stocks and being compared to a very well-diversified benchmark index.

\section*{Performing a $t$-Test on Regression Parameters}
The second major interpretation of a regression's results is testing the significance of the parameter estimates. In an application of Equation 1, the intercept of the regression is usually interpreted as an estimate of the ex ante alpha, or skill of the fund manager (if a fund's return is being analyzed), or the superior risk-adjusted return of a security (if a security's return is being analyzed). The slope coefficient of the regression is usually interpreted as the beta of the asset, a measure of the asset's systematic risk.

The parameter estimates of the regression are typically examined for statistical significance using a $t$-test. A $t$-test is a statistical test that rejects or fails to reject a hypothesis by comparing a $t$-statistic to a critical value.

For each alpha and beta estimate, the $t$-statistic is formed. The $t$-statistic of a parameter is formed by taking the estimated absolute value of the parameter and dividing by its standard error. The resulting $t$-statistic is compared to a critical value. If the $t$-statistic exceeds the critical value, the parameter estimate is deemed to be significantly different from zero. The critical value of the $t$-statistic is found from published lists of critical values based on two parameters: (1) the degrees of freedom and (2) the desired significance level of the test.


\end{document}