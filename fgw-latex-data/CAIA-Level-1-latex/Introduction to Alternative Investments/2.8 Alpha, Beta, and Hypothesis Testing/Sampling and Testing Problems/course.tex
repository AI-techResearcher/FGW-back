\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}

\begin{document}
\section*{Reading}
Sampling and Testing Problems

This section discusses potential problems when the sample being analyzed is not representative of the population or is not correctly interpreted.

\section*{Unrepresentative Data Sets}
The validity of a statistical analysis depends on the extent to which the sample or data set on which the analysis is performed is representative of the entire population for which the analyst is concerned. When a sample, subsample, or data set is a biased representation of the population, then statistical tests may be unreliable.

A bias is when a sample is obtained or selected in a manner that systematically favors inclusion of observations with particular characteristics that affect the statistical analysis.

For example, as privately placed investment pools, the total population or universe of hedge funds is unknown. Suppose that a researcher forms a sample of 100 funds for an in-depth analysis. If the 100 funds were selected at random, then the sample would be an unbiased representation of the population. However, if the 100 funds were selected on the basis of size or years in existence, then the sample would not be representative of the general hedge fund population. Statistical inferences about the entire population should not be made based on this biased sample with regard to such issues as return performance, since return performance is probably related to size and longevity. If the sample tends to contain established and large funds, the sample is likely to contain an upward bias in long-term returns, since these large, established funds probably became large and established by generating higher long-term returns. This is an example of selection bias. Selection bias is a distortion in relevant sample characteristics from the characteristics of the population, caused by the sampling method of selection or inclusion. If the selection bias originates from the decision of fund managers to report or not to report their returns, then the bias is referred to as a self-selection bias.

A number of other related biases have been recognized in alternative investment analysis, especially with regard to the construction of databases of hedge fund returns. For example, survivorship bias is a common problem in investment databases in which the sample is limited to those observations that continue to exist through the end of the period of study. Funds that liquidated, failed, or closed, perhaps due to poor returns, would be omitted.

\section*{Data Mining versus Data Dredging}
Data mining typically refers to the vigorous use of data to uncover valid relationships. ${ }^{1}$ The term data mining used to be commonly used to indicate overuse of data synonymous with data dredging. The idea is that by using a variety of well-designed statistical tests and exploring a number of data sources, analysts may uncover previously missed relationships. Data dredging, or data snooping, refers to the overuse and misuse of statistical tests to identify historical patterns. The difference is that data dredging involves performing too many tests, especially regarding historical relationships for which there are not a priori reasons for believing that the relationships reflect true causality. The problem with data dredging is not so much the number of tests performed as the failure to take the number of tests performed into account when analyzing the results.

The primary point is this: Any empirical results should be analyzed not only in the context of other research and economic reasoning but also through an understanding of how many tests have been performed. Not only can this information be difficult to obtain or estimate, but it may also be intentionally masked by researchers attempting to bolster a particular view.

\section*{Backtesting and Backfilling}
Backtesting is the use of historical data to test a strategy that was developed subsequent to the observation of the data. Backtesting can be a valid method of obtaining information on the historical risk and return of a strategy, which can be used as an indication of the strategy's potential going forward. However, backtesting combined with data dredging and numerous strategies can generate false indications of future returns. The reason is that the strategy identified as most successful in the past is likely to have had its performance driven by luck. One must be especially careful of allocating funds to investment managers who choose to report backtested results of their new model rather than the actual returns of the disappointing old model that traded client money in real time.

Backtesting is especially dangerous when the model involves overfitting. Overfitting is using too many parameters to fit a model very closely to data over some past time frame. Models that have been overfit tend to have a smaller chance of fitting future data than a model using fewer and more generalized parameters.

In alternative investments, backfilling typically refers to the insertion of an actual trading record of an investment into a database when that trading record predates the entry of the investment into the database. An example of backfilling would be the inclusion of a hedge fund into a database in 2015, along with the results of the fund since its inception in 2010.

Backfilling of actual results can be an appropriate technique, especially when done with full disclosure and when there is a reasonable basis to believe that the results will not create a substantial bias or deception. Thus, data sets of investment fund returns sometimes include past actual results of funds in the data set when the sample of funds being included is not being assembled on the basis of past investment results. The danger with backfilling is backfill bias. Backfill bias, or instant history bias, is when the funds, returns, and strategies being added to a data set are not representative of the universe of fund managers, fund returns, and fund strategies.

Instead, the additions would typically generate an upward return bias because it would be likely that the data set would disproportionately add the returns of successful funds that are more likely to survive and that may be more likely to want to publicize their results.

Backfilling can also refer to the use of hypothetical data from backtesting. In investments in general, backfilling sometimes refers to the insertion of hypothetical trading results into a summary of an investment opportunity. A reason that backfilling rarely refers to the inclusion of hypothetical trading results in the case of alternative investments is that alternative investments often focus on active trading strategies, in which hypothetical trading results would be highly discretionary and would be unsuited to hypothetical backfilling.

For example, an investment firm may have two funds with highly similar strategies, except that one fund uses two-to-one leverage and the other fund is unleveraged. Suppose that the unleveraged fund has been trading for 10 years and the leveraged fund has been trading for five years, and that, over the past five years, the\\
leveraged fund has shown a very consistent relationship to the unleveraged fund. If clearly disclosed as being hypothetical, it may be reasonable to indicate the 10year return that could have been expected if the leveraged fund had been in existence for 10 years, based on the observed relationship.

Backfilling can be deceptive even with innocent intentions. Often investors change or evolve their strategies as time passes, conditions change, and performance declines. Traders are especially likely to adapt their strategies in response to poor performance. An investor who backtests a revised trading strategy and backfills the hypothetical performance into the track record of the current and revised strategy is clearly providing a biased indication of forward-looking performance. The indication would be especially biased if the revision in the strategy were in response to data from the same time interval on which the backfilling was performed.

\section*{Cherry-Picking and Chumming}
Cherry-picking is the concept of extracting or publicizing only those results that support a particular viewpoint. Consider an investment manager who oversees 10 funds.

If the manager is not particularly skillful but takes large risks, half of the funds might be expected to outperform their benchmark in a given year, and half might be expected to underperform. After three or four years, there would probably be one fund with exceptionally high returns, and perhaps most of the remaining funds might be liquidated. Cherry-picking is the advertising and promotion of the results of the successful fund without adequately disclosing the number and magnitude of failed or poorly performing funds. If an investment firm has a large number of funds and is regularly opening new funds and closing old funds, it should be no surprise if many of the remaining funds are historical performance leaders.

Chumming is a fishing term used to describe scattering pieces of cheap fish into the water as bait to attract larger fish to catch. In investments, we apply this term to the practice of unscrupulous investment managers broadcasting a variety of predictions in the hope that some of them will turn out to be correct and thus be viewed as an indication of skill. For example, consider an unscrupulous Internet-based newsletter writer who sends 10 million emails, 5 million of which forecast that a particular stock will rise and 5 million of which forecast that it will fall. After observing whether the stock rises or falls, the writer sends follow-up emails to the 5 million recipients of the email with the predictions that were correct in retrospect. This second email notes the success of the previous prediction and makes another bold prediction. One version of that second email predicts that a second stock will rise and is sent to 2.5 million addresses, and an opposite prediction is sent to the remaining 2.5 million addresses. The process continues perhaps six or seven times until the writer has a list of 100,000 or so addresses that received six or seven correct predictions in a row. The people who received the string of correct predictions are encouraged to pay money for additional newsletter forecasts.

Would the recipient of six or seven correct predictions be persuaded that the results were generated by skill? Perhaps if the recipients understood that 9.9 million recipients received one or more bad predictions, it would be clear that the good predictions were based on luck. That is the key problem also observed in data dredging: Attractive results are usually not interpreted in the context of the total number of experiments being conducted.


\end{document}