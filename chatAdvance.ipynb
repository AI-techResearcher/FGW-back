{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using ChatGPT model (gpt-3.5-turbo)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "# loader = UnstructuredFileLoader(\"/Users/alphatech/Downloads/cnv_2024_04_08_3bd653f30b35923f0168g/cnv_2024_04_08_3bd653f30b35923f0168g.tex\")\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/course.pdf\n",
      "Successfully extracted content from: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/course.pdf\n",
      "Processing: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/Learning Objective.docx\n",
      "Skipping non-PDF file: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/Learning Objective.docx\n",
      "Processing: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/math.pdf\n",
      "Successfully extracted content from: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/math.pdf\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import PyPDF2\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "\n",
    "# def extract_text_from_pdf(pdf_file_path):\n",
    "    \n",
    "#     try:\n",
    "#         loader = PyPDFLoader(pdf_file_path)\n",
    "#         data = loader.load()\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading PDF file '{pdf_file_path}': {e}\")\n",
    "#     return data\n",
    "\n",
    "\n",
    "\n",
    "# def fetch_pdfs(root_folder, exam_name, topic_name, chapter_name, subchapter_name):\n",
    "#     exam_path = os.path.join(root_folder, exam_name)\n",
    "#     topic_path = os.path.join(exam_path, topic_name)\n",
    "#     chapter_path = os.path.join(topic_path, chapter_name)\n",
    "#     subchapter_path = os.path.join(chapter_path, subchapter_name)\n",
    "\n",
    "#     pdf_contents = []\n",
    "\n",
    "#     for subchapter, _, pdf_files in os.walk(subchapter_path):\n",
    "#         for pdf_file in pdf_files:\n",
    "#             pdf_path = os.path.join(subchapter, pdf_file)\n",
    "#             print(f\"Processing: {pdf_path}\")\n",
    "#             if pdf_file.endswith('.pdf'):  # Check if the file is a PDF\n",
    "#                 pdf_content = extract_text_from_pdf(pdf_path)\n",
    "#                 if pdf_content:\n",
    "#                     pdf_contents.extend(pdf_content)\n",
    "#                     print(f\"Successfully extracted content from: {pdf_path}\")\n",
    "#                 else:\n",
    "#                     print(f\"Failed to extract content from: {pdf_path}\")\n",
    "#             else:\n",
    "#                 print(f\"Skipping non-PDF file: {pdf_path}\")\n",
    "            \n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "#         pdf_texts = text_splitter.split_documents(pdf_contents)\n",
    "#     return pdf_texts\n",
    "\n",
    "# # Example usage:\n",
    "# root_folder = '/Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1'\n",
    "# exam_name = \"CAIA Level 1\"\n",
    "# topic_name = \"Hedge Funds\"\n",
    "# chapter_name = \"5.2 Macro and Managed Futures Funds\"\n",
    "# subchapter_name = \"Systematic Trading\"\n",
    "\n",
    "# pdf_contents = fetch_pdfs(root_folder, exam_name, topic_name, chapter_name, subchapter_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# loader = PyPDFLoader('/Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1')\n",
    "# data = loader.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Apr/2024 17:23:32] \"OPTIONS / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter is:  5.2 Macro and Managed Futures Funds\n",
      "topic is:  Hedge Funds\n",
      "subchapter is:  Systematic Trading\n",
      "Processing: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/course.pdf\n",
      "Successfully extracted content from: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/course.pdf\n",
      "Processing: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/Learning Objective.docx\n",
      "Skipping non-PDF file: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/Learning Objective.docx\n",
      "Processing: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/math.pdf\n",
      "Successfully extracted content from: /Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1/CAIA Level 1/Hedge Funds/5.2 Macro and Managed Futures Funds/Systematic Trading/math.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "127.0.0.1 - - [06/Apr/2024 17:23:45] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro funds are a type of hedge fund that focuses on making investment decisions based on macroeconomic trends and events. These funds typically take large positions in various asset classes such as stocks, bonds, currencies, and commodities based on their analysis of global economic factors.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS  # Import CORS\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "app = Flask(__name__)\n",
    "#run_with_ngrok(app)  # Initialize ngrok when the app is run\n",
    "\n",
    "CORS(app, supports_credentials=True, allow_headers=[\"Content-Type\"])\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "            \n",
    "            try:\n",
    "                loader = Docx2txtLoader(file_path)\n",
    "                docx_file = loader.load()\n",
    "                print(\"Docx file is: \", docx_file)\n",
    "                return docx_file\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading docx file '{file_path}': {e}\")\n",
    "\n",
    "def fetch_pdfs(root_folder, exam_name, topic_name, chapter_name, subchapter_name):\n",
    "    exam_path = os.path.join(root_folder, exam_name)\n",
    "    topic_path = os.path.join(exam_path, topic_name)\n",
    "    chapter_path = os.path.join(topic_path, chapter_name)\n",
    "    subchapter_path = os.path.join(chapter_path, subchapter_name)\n",
    "\n",
    "    pdf_texts = []\n",
    "    pdf_contents = []\n",
    "\n",
    "    for subchapter, _, pdf_files in os.walk(subchapter_path):\n",
    "        for pdf_file in pdf_files:\n",
    "            pdf_path = os.path.join(subchapter, pdf_file)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            if pdf_file.endswith('.pdf'):  # Check if the file is a PDF\n",
    "                pdf_content = extract_text_from_pdf(pdf_path)\n",
    "                if pdf_content:\n",
    "                    pdf_contents.extend(pdf_content)\n",
    "                    print(f\"Successfully extracted content from: {pdf_path}\")\n",
    "                else:\n",
    "                    print(f\"Failed to extract content from: {pdf_path}\")\n",
    "            else:\n",
    "                print(f\"Skipping non-Docx file: {pdf_path}\")\n",
    "            \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        pdf_texts = text_splitter.split_documents(pdf_contents)\n",
    "    return pdf_texts\n",
    "        \n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def ask_question():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        question = data['question']\n",
    "        \n",
    "        chapter = data['subChapter']\n",
    "        print(\"chapter is: \", chapter)\n",
    "        \n",
    "        topic = data['chapter']\n",
    "        print(\"topic is: \", topic)\n",
    "        \n",
    "        subChapter = data['topic']\n",
    "        print(\"subchapter is: \", subChapter)\n",
    "        \n",
    "        if question is None or question.strip() == '':\n",
    "                raise ValueError(\"Invalid or empty question\")\n",
    "        \n",
    "        root_folder = '/Users/alphatech/Desktop/Educational web app/fgwpro-main2/DataTheory/CAIA Level 1'\n",
    "        exam_name = \"CAIA Level 1\"\n",
    "        \n",
    "        pdf_contents = fetch_pdfs(root_folder, exam_name, topic, chapter, subChapter)\n",
    "\n",
    "        \n",
    "        chat_model = ChatOpenAI(temperature=0, model_name = 'gpt-3.5-turbo')\n",
    "\n",
    "        template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        Ensure the answer is correct and its explaination to be conforming to the context.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Helpful Answer:\"\"\"\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        \n",
    "        \n",
    "\n",
    "        model_name = \"BAAI/bge-small-en\"\n",
    "        model_kwargs = {\"device\": \"cpu\"}\n",
    "        encode_kwargs = {\"normalize_embeddings\": True}\n",
    "        hf = HuggingFaceBgeEmbeddings(\n",
    "            model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "        )\n",
    "\n",
    "        \n",
    "        vectorstore = FAISS.from_documents(pdf_contents, embedding=hf)\n",
    "        \n",
    "        retriever = vectorstore.as_retriever()\n",
    "        \n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | chat_model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        ans = chain.invoke(question)\n",
    "        print(ans)\n",
    "    \n",
    "        if ans is not None:\n",
    "            return jsonify({\"result\": ans})\n",
    "        else:\n",
    "            return jsonify({\"result\": \"No answer found\"})\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", str(e))\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
